name: Tests and Quality Checks

on:
  push:
    branches: [ main, develop, homolog ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    name: Code Quality Checks

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Run ruff linter
      run: |
        ruff check . --output-format=github

    - name: Check code formatting with black
      run: |
        black --check --diff .

    - name: Type checking with mypy
      run: |
        mypy . --ignore-missing-imports

    - name: Security analysis with bandit
      run: |
        bandit -r server.py src/ -f json -o bandit-report.json
        bandit -r server.py src/ -f txt

    - name: Upload security report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-report
        path: bandit-report.json

  unit-tests:
    runs-on: ubuntu-latest
    name: Unit Tests
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Run unit tests with coverage
      run: |
        pytest tests/unit \
          --cov=server \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=junit.xml \
          --html=reports/pytest-report.html \
          --self-contained-html \
          -v

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          junit.xml
          htmlcov/
          reports/

  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    services:
      firebird:
        image: jacobalberty/firebird:v4.0
        env:
          FIREBIRD_DATABASE: test.fdb
          FIREBIRD_USER: SYSDBA
          FIREBIRD_PASSWORD: test123
          ISC_PASSWORD: test123
        ports:
          - 3050:3050
        options: >-
          --health-cmd "/usr/local/firebird/bin/fbstat -h /firebird/data/test.fdb"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
          --health-start-period 30s

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Wait for Firebird to be ready
      run: |
        for i in {1..30}; do
          if nc -z localhost 3050; then
            echo "Firebird is ready"
            break
          fi
          echo "Waiting for Firebird... ($i/30)"
          sleep 10
        done

    - name: Run integration tests
      env:
        FIREBIRD_HOST: localhost
        FIREBIRD_PORT: 3050
        FIREBIRD_DATABASE: /firebird/data/test.fdb
        FIREBIRD_USER: SYSDBA
        FIREBIRD_PASSWORD: test123
        FIREBIRD_CHARSET: UTF8
      run: |
        pytest tests/integration \
          --junit-xml=integration-junit.xml \
          --html=reports/integration-report.html \
          --self-contained-html \
          -v \
          -x

    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-junit.xml
          reports/integration-report.html

  performance-tests:
    runs-on: ubuntu-latest
    name: Performance Tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
        pip install pytest-benchmark

    - name: Run performance tests
      run: |
        pytest tests/unit/test_performance.py \
          --benchmark-only \
          --benchmark-json=benchmark.json \
          --benchmark-sort=mean \
          -v

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark.json

  docker-build-test:
    runs-on: ubuntu-latest
    name: Docker Build Test

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      run: |
        docker build -t mcp-firebird-test .

    - name: Test Docker image
      run: |
        # Test that image starts correctly
        docker run --rm -d --name mcp-test mcp-firebird-test
        sleep 5
        
        # Check if container is still running
        if docker ps | grep mcp-test; then
          echo "Container started successfully"
          docker stop mcp-test
        else
          echo "Container failed to start"
          docker logs mcp-test
          exit 1
        fi

    - name: Test with environment variables
      run: |
        docker run --rm \
          -e FIREBIRD_HOST=test-host \
          -e FIREBIRD_DATABASE=/test.fdb \
          -e FIREBIRD_USER=TEST_USER \
          -e FIREBIRD_PASSWORD=test123 \
          mcp-firebird-test \
          python3 -c "import server; print('Import successful')"

  security-scan:
    runs-on: ubuntu-latest
    name: Security Scan

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  pre-commit:
    runs-on: ubuntu-latest
    name: Pre-commit Hooks

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install pre-commit
      run: |
        python -m pip install --upgrade pip
        pip install pre-commit

    - name: Run pre-commit hooks
      run: |
        pre-commit run --all-files --show-diff-on-failure

  test-summary:
    runs-on: ubuntu-latest
    name: Test Summary
    needs: [lint-and-format, unit-tests, integration-tests, docker-build-test]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Display test summary
      run: |
        echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check job results
        LINT_RESULT="${{ needs.lint-and-format.result }}"
        UNIT_RESULT="${{ needs.unit-tests.result }}"
        INTEGRATION_RESULT="${{ needs.integration-tests.result }}"
        DOCKER_RESULT="${{ needs.docker-build-test.result }}"
        
        echo "| Test Type | Result |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | $LINT_RESULT |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | $UNIT_RESULT |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | $INTEGRATION_RESULT |" >> $GITHUB_STEP_SUMMARY
        echo "| Docker Build | $DOCKER_RESULT |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check if any test failed
        if [[ "$LINT_RESULT" != "success" || "$UNIT_RESULT" != "success" || "$INTEGRATION_RESULT" != "success" || "$DOCKER_RESULT" != "success" ]]; then
          echo "❌ Some tests failed. Please check the individual job results." >> $GITHUB_STEP_SUMMARY
          exit 1
        else
          echo "✅ All tests passed successfully!" >> $GITHUB_STEP_SUMMARY
        fi
